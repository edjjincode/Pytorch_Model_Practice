{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSPNet을 활용한 물체 감지 흐름\n",
    "\n",
    "PSPNet을 활용한 시맨틱 분할의 4단계 흐름:\n",
    "\n",
    "1. 1단계에서는 전처리로 화상 크기를 475 * 475 픽셀로 리사이즈하고 색상 정보를 표준화 한다.\n",
    "\n",
    "2. 2단계에서는 PSPNet 신경망에 전처리한 화상을 입력한다. PSPNet 출력으로 21by475by475(클래스 수, 높이, 폭)의 배열이 출력된다. 출력 배열의 값은 픽셀이 해당 클래스일 신뢰도에 대응한 값이다.\n",
    "\n",
    "3. PSPNet 출력 값에 픽셀별로 신뢰도가 가장 높은 클래스와 각 픽셀이 대응할 것으로 예상되는 클래스를 구한다. 픽셀별 신뢰도가 최고로 높은 클래스 정보가 시맨틱 분할의 출력이 된다.\n",
    "\n",
    "4. 4단계에서는 시맨틱 분할의 출력을 입력 화상의 원 크기로 리사이즈한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from PIL import Image\n",
    "\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datapath_list(rootpath):\n",
    "\n",
    "    \"\"\"\n",
    "    학습 및 검증용 화상 데이터와 어노테이션 데이터의 파일 경로 리스트 작성\n",
    "    \"\"\"\n",
    "\n",
    "    # 화상 파일과 어노테이션 파일의 경로 템플릿 작성\n",
    "    imgpath_template = osp.join(rootpath, 'JPEGImages', '%s.jpg')\n",
    "    annopath_template = osp.join(rootpath, 'SegmentationClass', '%s.png')\n",
    "\n",
    "    # 훈련 및 검증 파일 각각의 ID 취득\n",
    "    train_id_names = osp.join(rootpath + 'ImageSets/Segmentation/train.txt')\n",
    "    val_id_names = osp.join(rootpath + 'ImageSets/Segmentation/val.txt')\n",
    "\n",
    "    #훈련 데이터의 화상 파일과 어노테이션 파일의 경로 리스트 작성\n",
    "    train_img_list = list()\n",
    "    train_anno_list = list()\n",
    "\n",
    "    for line in open(train_id_names):\n",
    "        file_id = line.strip()\n",
    "        img_path = (imgpath_template % file_id)\n",
    "        anno_path = (annopath_template % file_id)\n",
    "        train_img_list.append(img_path)\n",
    "        train_anno_list.append(anno_path)\n",
    "\n",
    "    val_img_list = list()\n",
    "    val_anno_list = list()\n",
    "\n",
    "    for line in open(val_id_names):\n",
    "        file_id = line.strip()\n",
    "        img_path = (imgpath_template % file_id)\n",
    "        anno_path = (annopath_template % file_id)\n",
    "        val_img_list.append(img_path)\n",
    "        val_anno_list.append(anno_path)\n",
    "    \n",
    "    return train_img_list, train_anno_list, val_img_list, val_anno_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootpath = \"./data/VOCdevkit/VOC2012/\"\n",
    "\n",
    "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(\n",
    "    rootpath=rootpath)\n",
    "\n",
    "print(train_img_list[0])\n",
    "print(train_anno_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset 클래스를 작성하기 전 화상과 어노테이션을 전처리하는 DataTransform 클래스를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_augumentation import Compose, Scale, RandomRotation, RandomMirror, Resize, Normalize_Tensor \n",
    "\n",
    "\n",
    "class DataTransform():\n",
    "\n",
    "    def __init__(self, input_size, color_mean, color_std):\n",
    "        self.data_transform = {\n",
    "            \"train\": Compose([\n",
    "                Scale(scale=[0.5, 1.5]),  \n",
    "                RandomRotation(angle=[-10, 10]),  \n",
    "                RandomMirror(), \n",
    "                Resize(input_size),  \n",
    "                Normalize_Tensor(color_mean, color_std) \n",
    "            ]),\n",
    "            'val': Compose([\n",
    "                Resize(input_size),  \n",
    "                Normalize_Tensor(color_mean, color_std)  \n",
    "            ])\n",
    "        }\n",
    "    \n",
    "    def __call__(self, phase, img, anno_class_img):\n",
    "\n",
    "        return self.data_transform[phase](img, anno_class_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset 클래스인 VOCDataset 클래스를 작성한다. VOCDataset 인스턴스 생성 시 화상 데이터  리스트, 어노테이션 데이터 리스트, 학습인지 검증인지 나타내는 phase 변수, 그리고 전처리 클래스의 인스턴스를 인수로 받는다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCDataset(data.Dataset):\n",
    "    \n",
    "\n",
    "    def __init__(self, img_list, anno_list, phase, transform):\n",
    "        self.img_list = img_list\n",
    "        self.anno_list = anno_list\n",
    "        self.phase = phase\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "       \n",
    "        img, anno_class_img = self.pull_item(index)\n",
    "        return img, anno_class_img\n",
    "\n",
    "    def pull_item(self, index):\n",
    "\n",
    "        image_file_path = self.img_list[index]\n",
    "        img = Image.open(image_file_path)   \n",
    "\n",
    "        anno_file_path = self.anno_list[index]\n",
    "        anno_class_img = Image.open(anno_file_path)  \n",
    "\n",
    "        img, anno_class_img = self.transform(self.phase, img, anno_class_img)\n",
    "\n",
    "        return img, anno_class_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGB 색의 평균치와 표준편차\n",
    "color_mean = (0.485, 0.456, 0.406)\n",
    "color_std = (0.229, 0.224, 0.225)\n",
    "\n",
    "# 데이터 셋 작성\n",
    "train_dataset = VOCDataset(train_img_list, train_anno_list, phase=\"train\", transform=DataTransform(\n",
    "    input_size=475, color_mean=color_mean, color_std=color_std))\n",
    "\n",
    "val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", transform=DataTransform(\n",
    "    input_size=475, color_mean=color_mean, color_std=color_std))\n",
    "\n",
    "print(val_dataset.__getitem__(0)[0].shape)\n",
    "print(val_dataset.__getitem__(0)[1].shape)\n",
    "print(val_dataset.__getitem__(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader를 만든다. DataLoader 작성 방법은 Object Detection과 동일하다. 하지만 Object Detection과 달리 어노테이션 데이터 크기가 데이터마다 변하지 않아 파이토치의 DataLoader 클래스를 그대로 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "train_dataloader = data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataloader = data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}\n",
    "\n",
    "batch_iterator = iter(dataloaders_dict[\"val\"]) \n",
    "imges, anno_class_imges = next(batch_iterator)  \n",
    "print(imges.size()) \n",
    "print(anno_class_imges.size())  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
